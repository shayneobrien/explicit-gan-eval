{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(100, 384)\n",
    "B = torch.rand(100, 384)\n",
    "# A = A.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_info(tensor, q1, q2):\n",
    "    reshaped = tensor.view(-1)\n",
    "    vals, _ = torch.sort(reshaped)\n",
    "    lower_index = torch.tensor(len(vals)*(q1/100.), dtype=torch.long)\n",
    "    upper_index = torch.tensor(len(vals)*(q2/100.), dtype=torch.long)\n",
    "    iqr, r = vals[upper_index]-vals[lower_index], max(reshaped)-min(reshaped)\n",
    "    return iqr, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr, r = pdf_info(A, 25, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iqr > 1e-5:\n",
    "    bin_width = 2*iqr/np.cbrt(samples)\n",
    "    bins = int(torch.round(r/bin_width))\n",
    "else:\n",
    "    # MNIST (since it's really only supposed to be either 0 or 1 as output)\n",
    "    # TODO: bin number\n",
    "    bins = 2\n",
    "\n",
    "# Bin data\n",
    "x = []\n",
    "for i in range(A.shape[1]):\n",
    "    x.append(torch.histc(A[:, i].unsqueeze(0), bins=bins))\n",
    "    \n",
    "x = torch.stack(x, dim=0).t()\n",
    "x[x == 0.] = .0001\n",
    "# res = np.array(x).T\n",
    "# res[res == 0] = .00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function histc:\n",
      "\n",
      "histc(...)\n",
      "    histc(input, bins=100, min=0, max=0, out=None) -> Tensor\n",
      "    \n",
      "    Computes the histogram of a tensor.\n",
      "    \n",
      "    The elements are sorted into equal width bins between :attr:`min` and\n",
      "    :attr:`max`. If :attr:`min` and :attr:`max` are both zero, the minimum and\n",
      "    maximum values of the data are used.\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        bins (int): number of histogram bins\n",
      "        min (int): lower end of the range (inclusive)\n",
      "        max (int): upper end of the range (inclusive)\n",
      "        out (Tensor, optional): the output tensor\n",
      "    \n",
      "    Returns:\n",
      "        Tensor: Histogram represented as a tensor\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\n",
      "        tensor([ 0.,  2.,  1.,  0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.histc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17., 25., 18.,  ..., 24., 22., 23.],\n",
       "        [13., 15., 23.,  ..., 18., 12., 22.],\n",
       "        [25., 18., 19.,  ..., 26., 14., 19.],\n",
       "        [29., 19., 22.,  ..., 14., 34., 19.],\n",
       "        [16., 23., 18.,  ..., 18., 18., 17.]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics for different archtypes\n",
    "def compute_divergences(A, B):\n",
    "    \"\"\" Compute divergence metrics (Jensen Shannon, Kullback-Liebler,\n",
    "    Wasserstein Distance, Energy Distance) between predicted distribution A\n",
    "    and true distribution B \"\"\"\n",
    "\n",
    "    # Get number of samples, IQR statistics, range\n",
    "    samples = A.shape[0]\n",
    "    iqr = np.percentile(A, 75)-np.percentile(A, 25)\n",
    "    r = np.max(A) - np.min(A)\n",
    "\n",
    "    # Get PDFs of predicted distribution A, true distribution B\n",
    "    B = get_pdf(B, iqr, r, samples)\n",
    "    A = get_pdf(A, iqr, r, samples)\n",
    "    \n",
    "    return A, B\n",
    "\n",
    "    # Mean\n",
    "#     m = (np.array(A)+np.array(B))/2\n",
    "\n",
    "#     # Compute metrics\n",
    "#     kl = entropy(pk=A, qk=B).sum()/A.shape[1]\n",
    "#     js = .5*(entropy(pk=A, qk=m)+entropy(pk=B, qk=m)).sum()/A.shape[1]\n",
    "#     wd = sum([wasserstein_distance(A[:,i], B[:,i]) for i in range(A.shape[1])])\n",
    "#     ed = sum([energy_distance(A[:,i], B[:,i]) for i in range(A.shape[1])])\n",
    "\n",
    "#     divergences = {\"KL-Divergence\": kl,\n",
    "#                     \"Jensen-Shannon\": js,\n",
    "#                     \"Wasserstein-Distance\": wd,\n",
    "#                     \"Energy-Distance\": ed}\n",
    "\n",
    "#     return divergences\n",
    "\n",
    "\n",
    "def get_pdf(data, iqr, r, samples):\n",
    "    \"\"\" Compute optimally binned probability distribution function  \"\"\"\n",
    "    x = []\n",
    "\n",
    "    if iqr > 1e-5:\n",
    "        bin_width = 2*iqr/np.cbrt(samples)\n",
    "        bins = int(round(r/bin_width, 0))\n",
    "    else:\n",
    "        # MNIST (since it's really only supposed to be either 0 or 1 as output)\n",
    "        # TODO: bin number\n",
    "        bins = 2\n",
    "\n",
    "    # Bin data\n",
    "    for i in range(data.shape[1]):\n",
    "        x.append(list(np.histogram(data[:, i], bins=bins, density=True)[0]))\n",
    "    \n",
    "    res = np.array(x).T\n",
    "    res[res == 0] = .00001\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(100, 384)\n",
    "B = torch.rand(100, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B  = compute_divergences(A.numpy(), B.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87283418, 1.25912741, 0.94811558, ..., 1.20506506, 1.10345383,\n",
       "        1.17052257],\n",
       "       [0.66746143, 0.75547645, 1.21148102, ..., 0.90379879, 0.60188391,\n",
       "        1.11963028],\n",
       "       [1.28357967, 0.90657174, 1.00078867, ..., 1.30548715, 0.70219789,\n",
       "        0.96695343],\n",
       "       [1.48895242, 0.95693683, 1.15880793, ..., 0.70295462, 1.70533774,\n",
       "        0.96695343],\n",
       "       [0.82149099, 1.15839722, 0.94811558, ..., 0.90379879, 0.90282586,\n",
       "        0.86516885]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function histogram in module numpy.lib.function_base:\n",
      "\n",
      "histogram(a, bins=10, range=None, normed=False, weights=None, density=None)\n",
      "    Compute the histogram of a set of data.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input data. The histogram is computed over the flattened array.\n",
      "    bins : int or sequence of scalars or str, optional\n",
      "        If `bins` is an int, it defines the number of equal-width\n",
      "        bins in the given range (10, by default). If `bins` is a\n",
      "        sequence, it defines the bin edges, including the rightmost\n",
      "        edge, allowing for non-uniform bin widths.\n",
      "    \n",
      "        .. versionadded:: 1.11.0\n",
      "    \n",
      "        If `bins` is a string from the list below, `histogram` will use\n",
      "        the method chosen to calculate the optimal bin width and\n",
      "        consequently the number of bins (see `Notes` for more detail on\n",
      "        the estimators) from the data that falls within the requested\n",
      "        range. While the bin width will be optimal for the actual data\n",
      "        in the range, the number of bins will be computed to fill the\n",
      "        entire range, including the empty portions. For visualisation,\n",
      "        using the 'auto' option is suggested. Weighted data is not\n",
      "        supported for automated bin size selection.\n",
      "    \n",
      "        'auto'\n",
      "            Maximum of the 'sturges' and 'fd' estimators. Provides good\n",
      "            all around performance.\n",
      "    \n",
      "        'fd' (Freedman Diaconis Estimator)\n",
      "            Robust (resilient to outliers) estimator that takes into\n",
      "            account data variability and data size.\n",
      "    \n",
      "        'doane'\n",
      "            An improved version of Sturges' estimator that works better\n",
      "            with non-normal datasets.\n",
      "    \n",
      "        'scott'\n",
      "            Less robust estimator that that takes into account data\n",
      "            variability and data size.\n",
      "    \n",
      "        'rice'\n",
      "            Estimator does not take variability into account, only data\n",
      "            size. Commonly overestimates number of bins required.\n",
      "    \n",
      "        'sturges'\n",
      "            R's default method, only accounts for data size. Only\n",
      "            optimal for gaussian data and underestimates number of bins\n",
      "            for large non-gaussian datasets.\n",
      "    \n",
      "        'sqrt'\n",
      "            Square root (of data size) estimator, used by Excel and\n",
      "            other programs for its speed and simplicity.\n",
      "    \n",
      "    range : (float, float), optional\n",
      "        The lower and upper range of the bins.  If not provided, range\n",
      "        is simply ``(a.min(), a.max())``.  Values outside the range are\n",
      "        ignored. The first element of the range must be less than or\n",
      "        equal to the second. `range` affects the automatic bin\n",
      "        computation as well. While bin width is computed to be optimal\n",
      "        based on the actual data within `range`, the bin count will fill\n",
      "        the entire range including portions containing no data.\n",
      "    normed : bool, optional\n",
      "        This keyword is deprecated in NumPy 1.6.0 due to confusing/buggy\n",
      "        behavior. It will be removed in NumPy 2.0.0. Use the ``density``\n",
      "        keyword instead. If ``False``, the result will contain the\n",
      "        number of samples in each bin. If ``True``, the result is the\n",
      "        value of the probability *density* function at the bin,\n",
      "        normalized such that the *integral* over the range is 1. Note\n",
      "        that this latter behavior is known to be buggy with unequal bin\n",
      "        widths; use ``density`` instead.\n",
      "    weights : array_like, optional\n",
      "        An array of weights, of the same shape as `a`.  Each value in\n",
      "        `a` only contributes its associated weight towards the bin count\n",
      "        (instead of 1). If `density` is True, the weights are\n",
      "        normalized, so that the integral of the density over the range\n",
      "        remains 1.\n",
      "    density : bool, optional\n",
      "        If ``False``, the result will contain the number of samples in\n",
      "        each bin. If ``True``, the result is the value of the\n",
      "        probability *density* function at the bin, normalized such that\n",
      "        the *integral* over the range is 1. Note that the sum of the\n",
      "        histogram values will not be equal to 1 unless bins of unity\n",
      "        width are chosen; it is not a probability *mass* function.\n",
      "    \n",
      "        Overrides the ``normed`` keyword if given.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    hist : array\n",
      "        The values of the histogram. See `density` and `weights` for a\n",
      "        description of the possible semantics.\n",
      "    bin_edges : array of dtype float\n",
      "        Return the bin edges ``(length(hist)+1)``.\n",
      "    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    histogramdd, bincount, searchsorted, digitize\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    All but the last (righthand-most) bin is half-open.  In other words,\n",
      "    if `bins` is::\n",
      "    \n",
      "      [1, 2, 3, 4]\n",
      "    \n",
      "    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n",
      "    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n",
      "    *includes* 4.\n",
      "    \n",
      "    .. versionadded:: 1.11.0\n",
      "    \n",
      "    The methods to estimate the optimal number of bins are well founded\n",
      "    in literature, and are inspired by the choices R provides for\n",
      "    histogram visualisation. Note that having the number of bins\n",
      "    proportional to :math:`n^{1/3}` is asymptotically optimal, which is\n",
      "    why it appears in most estimators. These are simply plug-in methods\n",
      "    that give good starting points for number of bins. In the equations\n",
      "    below, :math:`h` is the binwidth and :math:`n_h` is the number of\n",
      "    bins. All estimators that compute bin counts are recast to bin width\n",
      "    using the `ptp` of the data. The final bin count is obtained from\n",
      "    ``np.round(np.ceil(range / h))`.\n",
      "    \n",
      "    'Auto' (maximum of the 'Sturges' and 'FD' estimators)\n",
      "        A compromise to get a good value. For small datasets the Sturges\n",
      "        value will usually be chosen, while larger datasets will usually\n",
      "        default to FD.  Avoids the overly conservative behaviour of FD\n",
      "        and Sturges for small and large datasets respectively.\n",
      "        Switchover point is usually :math:`a.size \\approx 1000`.\n",
      "    \n",
      "    'FD' (Freedman Diaconis Estimator)\n",
      "        .. math:: h = 2 \\frac{IQR}{n^{1/3}}\n",
      "    \n",
      "        The binwidth is proportional to the interquartile range (IQR)\n",
      "        and inversely proportional to cube root of a.size. Can be too\n",
      "        conservative for small datasets, but is quite good for large\n",
      "        datasets. The IQR is very robust to outliers.\n",
      "    \n",
      "    'Scott'\n",
      "        .. math:: h = \\sigma \\sqrt[3]{\\frac{24 * \\sqrt{\\pi}}{n}}\n",
      "    \n",
      "        The binwidth is proportional to the standard deviation of the\n",
      "        data and inversely proportional to cube root of ``x.size``. Can\n",
      "        be too conservative for small datasets, but is quite good for\n",
      "        large datasets. The standard deviation is not very robust to\n",
      "        outliers. Values are very similar to the Freedman-Diaconis\n",
      "        estimator in the absence of outliers.\n",
      "    \n",
      "    'Rice'\n",
      "        .. math:: n_h = 2n^{1/3}\n",
      "    \n",
      "        The number of bins is only proportional to cube root of\n",
      "        ``a.size``. It tends to overestimate the number of bins and it\n",
      "        does not take into account data variability.\n",
      "    \n",
      "    'Sturges'\n",
      "        .. math:: n_h = \\log _{2}n+1\n",
      "    \n",
      "        The number of bins is the base 2 log of ``a.size``.  This\n",
      "        estimator assumes normality of data and is too conservative for\n",
      "        larger, non-normal datasets. This is the default method in R's\n",
      "        ``hist`` method.\n",
      "    \n",
      "    'Doane'\n",
      "        .. math:: n_h = 1 + \\log_{2}(n) +\n",
      "                        \\log_{2}(1 + \\frac{|g_1|}{\\sigma_{g_1}})\n",
      "    \n",
      "            g_1 = mean[(\\frac{x - \\mu}{\\sigma})^3]\n",
      "    \n",
      "            \\sigma_{g_1} = \\sqrt{\\frac{6(n - 2)}{(n + 1)(n + 3)}}\n",
      "    \n",
      "        An improved version of Sturges' formula that produces better\n",
      "        estimates for non-normal datasets. This estimator attempts to\n",
      "        account for the skew of the data.\n",
      "    \n",
      "    'Sqrt'\n",
      "        .. math:: n_h = \\sqrt n\n",
      "        The simplest and fastest estimator. Only takes into account the\n",
      "        data size.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.histogram([1, 2, 1], bins=[0, 1, 2, 3])\n",
      "    (array([0, 2, 1]), array([0, 1, 2, 3]))\n",
      "    >>> np.histogram(np.arange(4), bins=np.arange(5), density=True)\n",
      "    (array([ 0.25,  0.25,  0.25,  0.25]), array([0, 1, 2, 3, 4]))\n",
      "    >>> np.histogram([[1, 2, 1], [1, 0, 1]], bins=[0,1,2,3])\n",
      "    (array([1, 4, 1]), array([0, 1, 2, 3]))\n",
      "    \n",
      "    >>> a = np.arange(5)\n",
      "    >>> hist, bin_edges = np.histogram(a, density=True)\n",
      "    >>> hist\n",
      "    array([ 0.5,  0. ,  0.5,  0. ,  0. ,  0.5,  0. ,  0.5,  0. ,  0.5])\n",
      "    >>> hist.sum()\n",
      "    2.4999999999999996\n",
      "    >>> np.sum(hist * np.diff(bin_edges))\n",
      "    1.0\n",
      "    \n",
      "    .. versionadded:: 1.11.0\n",
      "    \n",
      "    Automated Bin Selection Methods example, using 2 peak random data\n",
      "    with 2000 points:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> rng = np.random.RandomState(10)  # deterministic random data\n",
      "    >>> a = np.hstack((rng.normal(size=1000),\n",
      "    ...                rng.normal(loc=5, scale=2, size=1000)))\n",
      "    >>> plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n",
      "    >>> plt.title(\"Histogram with 'auto' bins\")\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
